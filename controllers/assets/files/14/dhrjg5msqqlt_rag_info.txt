ما هو التوليد المعزز بالاسترداد (Retrieval-Augmented Generation)؟
التوليد المعزز بالاسترداد (RAG) هو عملية تحسين مخرجات نموذج اللغة الكبير، لذلك فهو يشير إلى قاعدة معرفية موثوقة خارج مصادر بيانات التدريب الخاصة به قبل إنشاء ردّ. يتم تدريب نماذج اللغة الكبيرة (LLM) على كميات هائلة من البيانات وتستخدم مليارات المعلمات لتوليد مخرجات أصلية لمهام مثل الإجابة على الأسئلة وترجمة اللغات وإكمال الجمل. يعمل التوليد المعزز بالاسترداد على مشاركة القدرات القوية الخاصة بنماذج اللغة الكبيرة مع نطاقات محددة أو قاعدة المعرفة الداخلية للمؤسسة، كل ذلك بدون الحاجة إلى إعادة تدريب النموذج. إنه نهج فعال من حيث التكلفة لتحسين مخرجات نماذج اللغة الكبيرة لكي تبقى مناسبة ودقيقة ومفيدة في سياقات مختلفة.

لماذا يعتبر Retrieval-Augmented Generation مهمًا؟
LLMs هي تقنية الذكاء الاصطناعي الرئيسية (AI) التي تعمل على تشغيل روبوتات الدردشة الذكية وتطبيقات معالجة اللغات الطبيعية الأخرى (NLP). الهدف هو إنشاء روبوتات يمكنها الإجابة على أسئلة المستخدم في سياقات مختلفة من خلال الرجوع إلى مصادر المعرفة الموثوقة. لسوء الحظ، فإن طبيعة تقنية LLM تقدم عدم القدرة على التنبؤ في استجابات LLM. بالإضافة إلى ذلك، فإن بيانات تدريب LLM ثابتة وتقدم تاريخًا نهائيًا للمعرفة التي تمتلكها.

تشمل التحديات المعروفة لـ LLMs ما يلي:

تقديم معلومات خاطئة عندما لا تحتوي على الإجابة.
تقديم معلومات قديمة أو عامة عندما يتوقع المستخدم استجابة محددة وحديثة.
إنشاء استجابة من مصادر غير موثوقة.
إنشاء إجابات غير دقيقة بسبب الخلط بين المصطلحات، حيث تستخدم مصادر التدريب المختلفة نفس المصطلحات للتحدث عن أشياء مختلفة.
يمكنك التفكير في نموذج اللغة الكبيرة كموظف جديد متحمس للغاية يرفض البقاء على اطلاع بالأحداث الجارية ولكنه سيجيب دائمًا على كل سؤال بثقة مطلقة. لسوء الحظ، يمكن أن يؤثر هذا الموقف سلبًا على ثقة المستخدم وليس شيئًا تريد أن تحاكيه روبوتات الدردشة الخاصة بك!

RAG هي إحدى الطرق لحل بعض هذه التحديات. يقوم بإعادة توجيه LLM لاسترداد المعلومات ذات الصلة من مصادر المعرفة الموثوقة والمحددة مسبقًا. تتمتع المؤسسات بقدر أكبر من التحكم في إخراج النص الذي تم إنشاؤه، ويكتسب المستخدمون رؤى حول كيفية قيام LLM بإنشاء الاستجابة.

ما هي فوائد Retrieval-Augmented Generation؟
تجلب تقنية RAG العديد من الفوائد لجهود الذكاء الاصطناعي المولّد للمؤسسة.

التنفيذ الفعال من حيث التكلفة
يبدأ تطوير Chatbot عادةً باستخدام نموذج التأسيس. نماذج التأسيس (FMs) هي نماذج LLMs يمكن الوصول إليها بواسطة API ومدربة على مجموعة واسعة من البيانات المعممة وغير المصنفة. التكاليف الحسابية والمالية لإعادة تدريب FMs للمؤسسة أو المعلومات الخاصة بالمجال مرتفعة. RAG هو نهج أكثر فعالية من حيث التكلفة لتقديم بيانات جديدة إلى LLM. إنها تجعل تقنية الذكاء الاصطناعي المولّد (Generative AI) متاحة على نطاق أوسع وقابلة للاستخدام.

المعلومات الحالية
حتى إذا كانت مصادر بيانات التدريب الأصلية لبرنامج LLM مناسبة لاحتياجاتك، فمن الصعب الحفاظ على ملاءمتها. تسمح RAG للمطورين بتقديم أحدث الأبحاث أو الإحصاءات أو الأخبار للنماذج التوليدية. يمكنهم استخدام RAG لربط LLM مباشرة بموجزات الوسائط الاجتماعية المباشرة أو المواقع الإخبارية أو مصادر المعلومات الأخرى التي يتم تحديثها بشكل متكرر. يمكن لـ LLM بعد ذلك تقديم أحدث المعلومات للمستخدمين.

ثقة المستخدم المحسنة
يسمح RAG لـ LLM بتقديم معلومات دقيقة مع إسناد المصدر. يمكن أن يتضمن الإخراج اقتباسات أو مراجع للمصادر. يمكن للمستخدمين أيضًا البحث عن مستندات المصدر بأنفسهم إذا كانوا بحاجة إلى مزيد من التوضيح أو مزيد من التفاصيل. يمكن أن يؤدي ذلك إلى زيادة الثقة في حل الذكاء الاصطناعي المولّد.

المزيد من تحكم المطورين
مع RAG، يمكن للمطورين اختبار وتحسين تطبيقات الدردشة الخاصة بهم بشكل أكثر كفاءة. يمكنهم التحكم في مصادر معلومات LLM وتغييرها للتكيف مع المتطلبات المتغيرة أو الاستخدام متعدد الوظائف. يمكن للمطورين أيضًا تقييد استرداد المعلومات الحساسة إلى مستويات ترخيص مختلفة والتأكد من أن LLM يولد الاستجابات المناسبة. بالإضافة إلى ذلك، يمكنهم أيضًا استكشاف الأخطاء وإصلاحها وإجراء الإصلاحات إذا كانت LLM تشير إلى مصادر معلومات غير صحيحة لأسئلة محددة. يمكن للمؤسسات تنفيذ تقنية الذكاء الاصطناعي المولّد بثقة أكبر لمجموعة واسعة من التطبيقات.

كيف يعمل التوليد المعزز بالاسترداد (Retrieval-Augmented Generation)؟
بدون RAG، يأخذ LLM مدخلات المستخدم ويخلق استجابة بناءً على المعلومات التي تم التدريب عليها - أو ما يعرفه بالفعل. باستخدام RAG، يتم تقديم مكون استرداد المعلومات الذي يستخدم إدخال المستخدم لسحب المعلومات أولاً من مصدر بيانات جديد. يتم تقديم كل من استعلام المستخدم والمعلومات ذات الصلة إلى LLM. يستخدم LLM المعرفة الجديدة وبيانات التدريب الخاصة بها لإنشاء استجابات أفضل. توفر الأقسام التالية نظرة عامة على العملية.

إنشاء بيانات خارجية
تسمى البيانات الجديدة خارج مجموعة بيانات التدريب الأصلية لـ LLM البيانات الخارجية. يمكن أن تأتي من مصادر بيانات متعددة، مثل واجهات برمجة التطبيقات أو قواعد البيانات أو مستودعات المستندات. قد توجد البيانات بتنسيقات مختلفة مثل الملفات أو سجلات قاعدة البيانات أو النص الطويل. تقوم تقنية أخرى للذكاء الاصطناعي، تسمى تضمين نماذج اللغة، بتحويل البيانات إلى تمثيلات رقمية وتخزينها في قاعدة بيانات متجهة. تعمل هذه العملية على إنشاء مكتبة معرفية يمكن لنماذج الذكاء الاصطناعي المولّد فهمها.

استرجع المعلومات ذات الصلة
الخطوة التالية هي إجراء بحث ذي صلة. يتم تحويل استعلام المستخدم إلى تمثيل متجه ومطابقته مع قواعد بيانات vector. على سبيل المثال، ضع في اعتبارك روبوت محادثة ذكي يمكنه الإجابة على أسئلة الموارد البشرية للمؤسسة. إذا بحث أحد الموظفين، «ما مقدار الإجازة السنوية التي أحصل عليها؟» سيسترد النظام وثائق سياسة الإجازة السنوية جنبًا إلى جنب مع سجل الإجازة السابقة للموظف الفردي. سيتم إرجاع هذه المستندات المحددة لأنها ذات صلة كبيرة بما أدخله الموظف. تم حساب الصلة وتأسيسها باستخدام حسابات vector الرياضية والتمثيلات.

زيادة أوامر LLM
بعد ذلك، يعمل نموذج RAG على زيادة إدخال المستخدم (أو الأوامر) عن طريق إضافة البيانات المستردة ذات الصلة في السياق. تستخدم هذه الخطوة تقنيات هندسة الأوامر (هندسة التلقين) للتواصل بفعالية مع LLM. تسمح الأوامر المعززة لنماذج اللغات الكبيرة بإنشاء إجابة دقيقة لاستفسارات المستخدم.

تحديث البيانات الخارجية
قد يكون السؤال التالي - ماذا لو أصبحت البيانات الخارجية قديمة؟ للحفاظ على المعلومات الحالية للاسترجاع، قم بتحديث المستندات بشكل غير متزامن وتحديث التمثيل المضمن للمستندات. يمكنك القيام بذلك من خلال العمليات الآلية في الوقت الفعلي أو المعالجة الدورية المُجمّعة. يعد هذا تحديًا شائعًا في تحليلات البيانات - يمكن استخدام مناهج علوم البيانات المختلفة لإدارة التغيير.

ما الفرق بين Retrieval-Augmented Generation والبحث الدلالي؟
يعزز البحث الدلالي نتائج RAG للمؤسسات التي ترغب في إضافة مصادر معرفة خارجية واسعة إلى تطبيقات LLM الخاصة بها. تقوم المؤسسات الحديثة بتخزين كميات هائلة من المعلومات مثل الكتيبات والأسئلة الشائعة وتقارير البحث وأدلة خدمة العملاء ومستودعات مستندات الموارد البشرية عبر أنظمة مختلفة. يعد استرجاع السياق تحديًا على نطاق واسع وبالتالي يقلل من جودة الإنتاج التوليدي.

يمكن لتقنيات البحث الدلالي مسح قواعد البيانات الكبيرة للمعلومات المتباينة واسترداد البيانات بشكل أكثر دقة. على سبيل المثال، يمكنهم الإجابة على أسئلة مثل «كم تم إنفاقه على إصلاح الآلات في العام الماضي؟» من خلال ربط السؤال بالمستندات ذات الصلة وإرجاع نص معين بدلاً من نتائج البحث. يمكن للمطورين بعد ذلك استخدام هذه الإجابة لتوفير المزيد من السياق لـ LLM.

تنتج حلول البحث التقليدية أو الكلمات الرئيسية في RAG نتائج محدودة للمهام كثيفة المعرفة. يجب على المطورين أيضًا التعامل مع عمليات تضمين الكلمات وتقسيم المستندات والتعقيدات الأخرى أثناء إعداد بياناتهم يدويًا. في المقابل، تقوم تقنيات البحث الدلالي بجميع أعمال إعداد قاعدة المعرفة حتى لا يضطر المطورون إلى ذلك. كما أنها تنشئ مقاطع ذات صلة لغويًا وكلمات رمزية مرتبة حسب الصلة لتعظيم جودة حمولة RAG.

كيف يمكن لـ AWS دعم متطلبات Retrieval-Augmented Generation؟
Amazon Bedrock هي خدمة مُدارة بالكامل تقدم مجموعة مختارة من نماذج التأسيس عالية الأداء - إلى جانب مجموعة واسعة من الإمكانات - لبناء تطبيقات الذكاء الاصطناعي المولد مع تبسيط التطوير والحفاظ على الخصوصية والأمان. باستخدام قواعد المعرفة لـ Amazon Bedrock، يمكنك توصيل نماذج التأسيس (FMs) بمصادر البيانات الخاصة بك لـ RAG ببضع نقرات فقط. تتم معالجة جميع تحويلات Vector وعمليات الاسترداد وتوليد المخرجات المحسّنة تلقائيًا.

بالنسبة للمؤسسات التي تدير RAG الخاصة بها، فإن Amazon Kendra هي خدمة بحث مؤسسية عالية الدقة مدعومة بتعلم الآلة. إنه يوفر واجهة برمجة تطبيقات Kendra Retrieve محسّنة يمكنك استخدامها مع التصنيف الدلالي عالي الدقة من Amazon Kendra كمسترد مؤسسي لعمليات سير عمل RAG الخاصة بك. على سبيل المثال، باستخدام واجهة برمجة تطبيقات Retrieve، يمكنك:

استرجاع ما يصل إلى 100 مقطع ذي صلة لغويًا يصل كل منها إلى 200 كلمة رمزية، مرتبة حسب الصلة.
استخدم الموصلات المبنية مسبقًا لتقنيات البيانات الشائعة مثل خدمة التخزين البسيطة في Amazon وSharePoint وConfluence ومواقع الويب الأخرى.
دعم مجموعة واسعة من تنسيقات المستندات مثل HTML وWord وPowerPoint وPDF وExcel والملفات النصية.
تصفية الردود بناءً على تلك المستندات التي تسمح بها أذونات المستخدم النهائي.
تقدم Amazon أيضًا خيارات للمؤسسات التي ترغب في بناء المزيد من حلول الذكاء الاصطناعي المولّد المخصصة. Amazon SageMaker JumpStart هو مركز تعلم الآلة (ML) مع FMs والخوارزميات المضمنة وحلول ML المبنية مسبقًا والتي يمكنك نشرها ببضع نقرات فقط. يمكنك تسريع تنفيذ RAG بالرجوع إلى دفاتر SageMaker الحالية وأمثلة التعليمات البرمجية.


